{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Consistency Distillation — MicroDiT (EDM)\n",
        "\n",
        "Distill MicroDiT 0.5B teacher (30 steps) → student (4 steps) via Consistency Distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "# micro_diffusion уже добавлен в sys.path в следующей ячейке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "SCRIPTS_DIR = '/kaggle/input/datasets/albertdavletshin/consistency-distillation-micro-diffusion'\n",
        "sys.path.insert(0, SCRIPTS_DIR)\n",
        "os.chdir(SCRIPTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, copy, gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers.optimization import get_scheduler\n",
        "\n",
        "# micro_diffusion находится в /kaggle/input/datasets/albertdavletshin/micro-diffusers\n",
        "sys.path.insert(0, '/kaggle/input/datasets/albertdavletshin/micro-diffusers')\n",
        "from micro_diffusion.models.model import create_latent_diffusion\n",
        "\n",
        "from cd_utils import get_sigmas, cd_loss\n",
        "from dataset import get_dataloader\n",
        "from sampler import generate_images, run_parti_prompts_benchmark\n",
        "\n",
        "print(f\"torch {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}, {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEACHER_CHECKPOINT = \"dit_4_channel_0.5B_synthetic_data.pt\"\n",
        "DATA_DIR = \"data/journeydb_subset\"\n",
        "OUTPUT_DIR = \"output\"\n",
        "\n",
        "LATENT_RES = 64\n",
        "IN_CHANNELS = 4\n",
        "POS_INTERP_SCALE = 2.0\n",
        "\n",
        "NUM_TIMESTEPS = 50\n",
        "GUIDANCE_SCALE = 5.0\n",
        "LOSS_TYPE = \"huber\"\n",
        "HUBER_C = 0.001\n",
        "\n",
        "LEARNING_RATE = 1e-5\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_TRAIN_STEPS = 5000\n",
        "MIXED_PRECISION = \"fp16\"\n",
        "SEED = 42\n",
        "MAX_GRAD_NORM = 1.0\n",
        "LR_WARMUP_STEPS = 100\n",
        "RESOLUTION = 512\n",
        "\n",
        "VALIDATION_STEPS = 500\n",
        "SAVE_STEPS = 1000\n",
        "\n",
        "VALIDATION_PROMPTS = [\n",
        "    \"A beautiful sunset over mountains with a clear lake, highly detailed\",\n",
        "    \"Portrait of a girl with golden hair, 8k, masterpiece\",\n",
        "    \"Astronaut floating in space with Earth in background\",\n",
        "    \"A cute corgi puppy playing in autumn leaves, photograph\",\n",
        "]\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "Download 5k images from JourneyDB (streaming, no full dataset needed).\n",
        "May require `huggingface-cli login` if gated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "NUM_SAMPLES = 5000\n",
        "\n",
        "if os.path.exists(DATA_DIR) and os.path.exists(os.path.join(DATA_DIR, \"captions.json\")):\n",
        "    n = len(json.load(open(os.path.join(DATA_DIR, \"captions.json\"))))\n",
        "    print(f\"Dataset already exists: {n} samples in {DATA_DIR}\")\n",
        "else:\n",
        "    images_dir = os.path.join(DATA_DIR, \"images\")\n",
        "    os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Downloading {NUM_SAMPLES} samples from JourneyDB...\")\n",
        "    ds = load_dataset(\"JourneyDB/JourneyDB\", split=\"train\", streaming=True)\n",
        "\n",
        "    captions = {}\n",
        "    count = 0\n",
        "    for sample in tqdm(ds, total=NUM_SAMPLES):\n",
        "        if count >= NUM_SAMPLES:\n",
        "            break\n",
        "        try:\n",
        "            image = sample['image']\n",
        "            prompt = sample.get('prompt', sample.get('text', ''))\n",
        "            if not prompt or not isinstance(prompt, str):\n",
        "                continue\n",
        "            fname = f\"{count:06d}.jpg\"\n",
        "            if isinstance(image, Image.Image):\n",
        "                image = image.convert(\"RGB\")\n",
        "                if max(image.size) > 1024:\n",
        "                    image.thumbnail((1024, 1024), Image.LANCZOS)\n",
        "                image.save(os.path.join(images_dir, fname), quality=95)\n",
        "            else:\n",
        "                continue\n",
        "            captions[fname] = prompt\n",
        "            count += 1\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    with open(os.path.join(DATA_DIR, \"captions.json\"), 'w') as f:\n",
        "        json.dump(captions, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Saved {count} samples to {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(TEACHER_CHECKPOINT):\n",
        "    !wget -q https://huggingface.co/VSehwag24/MicroDiT/resolve/main/ckpts/dit_4_channel_0.5B_synthetic_data.pt\n",
        "\n",
        "model = create_latent_diffusion(latent_res=LATENT_RES, in_channels=IN_CHANNELS, pos_interp_scale=POS_INTERP_SCALE)\n",
        "model.dit.load_state_dict(torch.load(TEACHER_CHECKPOINT, map_location='cpu'))\n",
        "print(f\"DiT: {sum(p.numel() for p in model.dit.parameters()) / 1e6:.1f}M params\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# teacher: frozen copy\n",
        "teacher_dit = copy.deepcopy(model.dit)\n",
        "teacher_dit.requires_grad_(False)\n",
        "teacher_dit.eval()\n",
        "\n",
        "# student: trainable (same init)\n",
        "student_dit = model.dit\n",
        "student_dit.train()\n",
        "\n",
        "model.vae.requires_grad_(False)\n",
        "model.text_encoder.requires_grad_(False)\n",
        "\n",
        "vae = model.vae\n",
        "text_encoder = model.text_encoder\n",
        "tokenizer = model.tokenizer\n",
        "edm_config = model.edm_config\n",
        "latent_scale = model.latent_scale\n",
        "\n",
        "print(f\"sigma_min={edm_config.sigma_min}, sigma_max={edm_config.sigma_max}, sigma_data={edm_config.sigma_data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=MIXED_PRECISION,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        ")\n",
        "weight_dtype = torch.float16 if MIXED_PRECISION == \"fp16\" else torch.bfloat16\n",
        "\n",
        "optimizer = torch.optim.AdamW(student_dit.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8)\n",
        "lr_scheduler = get_scheduler(\"constant_with_warmup\", optimizer=optimizer,\n",
        "                              num_warmup_steps=LR_WARMUP_STEPS, num_training_steps=MAX_TRAIN_STEPS)\n",
        "\n",
        "student_dit, optimizer, lr_scheduler = accelerator.prepare(student_dit, optimizer, lr_scheduler)\n",
        "\n",
        "device = accelerator.device\n",
        "teacher_dit.to(device, dtype=weight_dtype)\n",
        "vae.to(device)\n",
        "text_encoder.to(device, dtype=weight_dtype)\n",
        "\n",
        "sigmas = get_sigmas(NUM_TIMESTEPS, sigma_min=edm_config.sigma_min,\n",
        "                    sigma_max=edm_config.sigma_max, rho=edm_config.rho, device=device)\n",
        "\n",
        "print(f\"device={device}, effective_bs={TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"sigmas: {NUM_TIMESTEPS} steps, [{sigmas[0]:.2f} -> {sigmas[-1]:.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(step):\n",
        "    student = accelerator.unwrap_model(student_dit)\n",
        "    student.eval()\n",
        "    val_dir = os.path.join(OUTPUT_DIR, f\"val_{step}\")\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    for i, prompt in enumerate(VALIDATION_PROMPTS):\n",
        "        imgs = generate_images(student, model, [prompt]*2, num_steps=4, seed=SEED+i, device=device)\n",
        "        save_image(imgs, os.path.join(val_dir, f\"p{i}.png\"), nrow=2)\n",
        "    student.train()\n",
        "    print(f\"  val images -> {val_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = get_dataloader(DATA_DIR, resolution=RESOLUTION, batch_size=TRAIN_BATCH_SIZE)\n",
        "train_iter = iter(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses = []\n",
        "progress_bar = tqdm(range(MAX_TRAIN_STEPS), desc=\"CD Training\")\n",
        "\n",
        "for step in progress_bar:\n",
        "    batch = next(train_iter)\n",
        "    images = batch['image'].to(device)\n",
        "    captions = batch['caption']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        latents = vae.encode(images.to(weight_dtype))['latent_dist'].sample().data * latent_scale\n",
        "        tokens = tokenizer.tokenize(captions)['input_ids'].to(device)\n",
        "        text_emb = text_encoder.encode(tokens)[0]\n",
        "\n",
        "    with accelerator.accumulate(student_dit):\n",
        "        student = accelerator.unwrap_model(student_dit)\n",
        "        loss = cd_loss(\n",
        "            student, teacher_dit, edm_config,\n",
        "            latents.float(), text_emb.float(),\n",
        "            sigmas, GUIDANCE_SCALE,\n",
        "            loss_type=LOSS_TYPE, huber_c=HUBER_C, weight_dtype=weight_dtype\n",
        "        )\n",
        "        accelerator.backward(loss)\n",
        "        if accelerator.sync_gradients:\n",
        "            accelerator.clip_grad_norm_(student_dit.parameters(), MAX_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    loss_val = loss.detach().item()\n",
        "    losses.append(loss_val)\n",
        "    progress_bar.set_postfix(loss=f\"{loss_val:.4f}\", lr=f\"{lr_scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    if (step + 1) % VALIDATION_STEPS == 0:\n",
        "        print(f\"\\nstep {step+1}, avg_loss={np.mean(losses[-VALIDATION_STEPS:]):.4f}\")\n",
        "        validate(step + 1)\n",
        "\n",
        "    if (step + 1) % SAVE_STEPS == 0:\n",
        "        ckpt = os.path.join(OUTPUT_DIR, f\"student_step_{step+1}.pt\")\n",
        "        torch.save(accelerator.unwrap_model(student_dit).state_dict(), ckpt)\n",
        "        print(f\"  saved {ckpt}\")\n",
        "\n",
        "final_ckpt = os.path.join(OUTPUT_DIR, \"student_dit_final.pt\")\n",
        "torch.save(accelerator.unwrap_model(student_dit).state_dict(), final_ckpt)\n",
        "print(f\"Done. Final checkpoint: {final_ckpt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses, alpha=0.3, label='Raw')\n",
        "w = min(100, len(losses) // 5)\n",
        "if w > 1:\n",
        "    sm = np.convolve(losses, np.ones(w)/w, mode='valid')\n",
        "    plt.plot(range(w-1, len(losses)), sm, label=f'Smooth (w={w})', color='red')\n",
        "plt.xlabel('Step'); plt.ylabel('Loss'); plt.title('CD Training Loss')\n",
        "plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'loss.png'), dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment to load from checkpoint:\n",
        "# model.dit.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, \"student_dit_final.pt\"), map_location='cpu'))\n",
        "# student_dit = model.dit\n",
        "\n",
        "student = accelerator.unwrap_model(student_dit)\n",
        "student.eval()\n",
        "student.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_prompts = [\n",
        "    \"A beautiful sunset over mountains with a clear lake\",\n",
        "    \"An elegant squirrel pirate on a ship\",\n",
        "    \"A photo of an astronaut riding a horse\",\n",
        "    \"A cute corgi puppy playing in autumn leaves\",\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(len(test_prompts), 4, figsize=(16, 4*len(test_prompts)))\n",
        "for row, prompt in enumerate(test_prompts):\n",
        "    for col, n_steps in enumerate([1, 2, 4, 8]):\n",
        "        imgs = generate_images(student, model, [prompt], num_steps=n_steps, seed=SEED, device=device)\n",
        "        axes[row, col].imshow(imgs[0].cpu().permute(1,2,0).numpy())\n",
        "        axes[row, col].set_title(f\"{n_steps} step{'s' if n_steps>1 else ''}\")\n",
        "        axes[row, col].axis('off')\n",
        "plt.suptitle('Steps comparison'); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'steps.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PartiPrompts Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = run_parti_prompts_benchmark(\n",
        "    student, model,\n",
        "    tsv_path=\"PartiPrompts.tsv\",\n",
        "    output_dir=os.path.join(OUTPUT_DIR, \"parti_prompts_4steps\"),\n",
        "    num_steps=4, num_prompts=100, batch_size=4, seed=2024, device=device,\n",
        ")\n",
        "print(f\"Generated {len(results)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "for idx, (prompt, img_path) in enumerate(results[:16]):\n",
        "    r, c = idx // 4, idx % 4\n",
        "    axes[r, c].imshow(Image.open(img_path))\n",
        "    axes[r, c].set_title(prompt[:50] + '...', fontsize=8)\n",
        "    axes[r, c].axis('off')\n",
        "plt.suptitle('PartiPrompts (4 steps)'); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'parti_grid.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Teacher vs Student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cmp_prompts = [\n",
        "    \"An elegant squirrel pirate on a ship\",\n",
        "    \"A photo of a cat wearing sunglasses\",\n",
        "    \"A beautiful mountain landscape at sunset\",\n",
        "    \"A robot couple fine dining with Eiffel Tower in the background\",\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(len(cmp_prompts), 2, figsize=(10, 5*len(cmp_prompts)))\n",
        "for row, prompt in enumerate(cmp_prompts):\n",
        "    t_img = model.generate(prompt=[prompt], num_inference_steps=30, guidance_scale=GUIDANCE_SCALE, seed=SEED)\n",
        "    s_img = generate_images(student, model, [prompt], num_steps=4, seed=SEED, device=device)\n",
        "\n",
        "    axes[row, 0].imshow(t_img[0].cpu().permute(1,2,0).numpy())\n",
        "    axes[row, 0].set_title('Teacher (30 steps)'); axes[row, 0].axis('off')\n",
        "    axes[row, 1].imshow(s_img[0].cpu().permute(1,2,0).numpy())\n",
        "    axes[row, 1].set_title('Student CD (4 steps)'); axes[row, 1].axis('off')\n",
        "\n",
        "plt.suptitle('Teacher (30) vs Student (4)'); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'comparison.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
